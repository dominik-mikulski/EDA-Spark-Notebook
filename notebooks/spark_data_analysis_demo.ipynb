{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a200ce-852f-44d1-9997-9db039bef82b",
   "metadata": {},
   "source": [
    "# Spark Data Analysis Demo\n",
    "This notebook is a demo of how you one can use spark for exploratory data analysis.stems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f209f4-6098-4b6f-9df2-717f997eeb18",
   "metadata": {},
   "source": [
    "## 1. Why this EDA exists\r\n",
    "\r\n",
    "In modern data platforms, data constantly crosses system boundaries:\r\n",
    "from applications to data distributors, from distributors to data lakes,\r\n",
    "from data lakes to warehouses, and from warehouses into analytics\r\n",
    "and machine-learning systems.\r\n",
    "\r\n",
    "At each of these transitions, data can become incomplete\r\n",
    "(e.g. broken exports), duplicative (e.g. repeated ingestimistreatednderstood (e.g. timestamps ingested as strings, numbers parsed as text).\r\n",
    "These issues often propagate silently and eventually break reports,\r\n",
    "analytics, and machine-learning models.\r\n",
    "\r\n",
    "This EDA exissuch detect those risks early, before the data is used\r\n",
    "in product\n",
    "\n",
    "This EDA does not cover ingestion errors (i.e. corrupted files or transmissions), it assumes data parsing completed without errors. ion pipelines. silent data corruption.\r\n",
    " be automated in production pipelines.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b3588b-5baf-430b-b2ec-b7d42291eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To explore data we need a spark session. Its and an object used to read data, explore data (including not limited to sql), access spark confirguration. \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-dq-demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaae1640-fec2-4548-b2a0-6b3d11f96b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets read a csv file into spark data frame, inferSchema will detect datatypes (if consistent), header will take first row as header.\n",
    "df = spark.read.csv(\n",
    "    \"../data/retail_personalization_dataset.csv\",\n",
    "    inferSchema=True,\n",
    "    header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3356488-1f3a-4bea-866e-694828746a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- interaction_type: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- discount: integer (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- user_age: integer (nullable = true)\n",
      " |-- user_gender: string (nullable = true)\n",
      " |-- loyalty_score: integer (nullable = true)\n",
      " |-- previous_purchase_count: integer (nullable = true)\n",
      " |-- avg_purchase_value: double (nullable = true)\n",
      " |-- search_keywords: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's print schema of the data frame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74da27a-6e9d-494e-a389-ab55dffde2a9",
   "metadata": {},
   "source": [
    "Because the source is a CSV file, there is no enforced schema or\r\n",
    "nullability contract, and all constraints must be validated in Spark.\r\n",
    "\r\n",
    "Headers were read correctly and key fields such as timestamp, price,\r\n",
    "discount, user_age, loyalty_score, previous_purchase_count,\r\n",
    "avg_purchase_value, rating, and purchase were correctly inferred as\r\n",
    "numeric or temporal types and not silently downgraded to strings due to\r\n",
    "inconsistent formatting.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c4112-4bf6-4fa7-88b2-18e1835d11f4",
   "metadata": {},
   "source": [
    "## 3. Event-level uniqueness and duplicate risk\n",
    "\n",
    "Before aggregating, joining, or building features, we must understand\n",
    "what a single row represents and whether rows are duplicated.\n",
    "\n",
    "In this dataset, each row is assumed to represent a single user\r\n",
    "interaction with a product at a given point in time.\r\n",
    "\r\n",
    "To validate this assumption, we perform two checks:\r\n",
    "\r\n",
    "- **Full-row duplicates**  \r\n",
    "  We check whether identical rows appear more than once. The presence\r\n",
    "  of full duplicates would indicate upstream ingestion or replay issues.\r\n",
    "\r\n",
    "- **Event-level duplicates**  \r\n",
    "  We check for duplicates using a proxy event identifier:\r\n",
    "  `(user_id, product_id, session_id, timestamp)`.  \r\n",
    "  If this combination is not unique, it suggests that the same real-world\r\n",
    "  interaction has been recorded more than once.\r\n",
    "\r\n",
    "Duplicates at either level would cause overcounting in reports,\r\n",
    "distort aggregated metrics, and introduce bias into machine-learning\r\n",
    "features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e0b806-3786-4a2c-b49e-00b684b42f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150002"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows=df.count()\n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22457a3c-52ba-4183-bf10-9fcf24c9d873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_rows=df.dropDuplicates().count()\n",
    "distinct_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db1e64ae-5269-4f52-bbb7-9abd2a961fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows = total_rows - distinct_rows\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc024975-8ae2-41fd-9117-d10ab8e93295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_keys=df.select(\"user_id\",\"product_id\",\"timestamp\",\"session_id\").dropDuplicates().count()\n",
    "rows_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f34f07-0e01-4231-ba5d-fd982d4fdd64",
   "metadata": {},
   "source": [
    "### Results\r\n",
    "\r\n",
    "The dataset contains:\r\n",
    "- 2 fully duplicated rows\r\n",
    "- 2 duplicated event records when using the proxy\r\n",
    "  `(user_id, product_id, session_id, timestamp)`\r\n",
    "\r\n",
    "This indicates that the same real-world interaction has been recorded\r\n",
    "more than once.\r\n",
    "\r\n",
    "While the number of duplicates is small, their presence confirms that\r\n",
    "duplicate events are a realistic failure mode and must be handled by\r\n",
    "t\n",
    "\n",
    "### Implications for data pipelines\r\n",
    "\r\n",
    "Because duplicate events exist and the dataset contains sufficient\r\n",
    "identifiers to detect them, this data can be made safe for downstream\r\n",
    "use by applying deterministic deduplication during ingestion.\r\n",
    "\r\n",
    "The following checks should be automated in production:\r\n",
    "- detection of fully duplicated rows\r\n",
    "- detection of duplicated event identifiers\r\n",
    "\r\n",
    "Ingestion should either:\r\n",
    "- drop duplicates, or\r\n",
    "- fail when duplicate rates exceed an acceptable thresholdhe pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80acc9a8-e844-4502-abca-817a852a0579",
   "metadata": {},
   "source": [
    "## 4. Completeness and missing data\n",
    "\n",
    "Missing values (Nulls, Nans) can silently corrupt aggregations, joins, and machine-learning\n",
    "features. This section evaluates how complete the dataset is and whether\n",
    "critical fields are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20d4a088-3654-429e-8922-ee8f42fc28bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get list of columns that are numeric types to later check NaN (Not a Number)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import NumericType\n",
    "numeric_cols=[f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42b40966-727c-4b6e-b508-e3ae19a47cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'price': 0,\n",
       " 'discount': 0,\n",
       " 'user_age': 0,\n",
       " 'loyalty_score': 0,\n",
       " 'previous_purchase_count': 0,\n",
       " 'avg_purchase_value': 0,\n",
       " 'rating': 0,\n",
       " 'purchase': 0}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dict of columns and their nan counts\n",
    "nan_count=df.select([F.count(F.when(F.isnan(F.col(c)),1)).alias(c) for c in numeric_cols])\n",
    "nan_count.first().asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1b1833a-df15-4de9-943e-b9dca1d3f653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_id': 0,\n",
       " 'product_id': 0,\n",
       " 'timestamp': 0,\n",
       " 'session_id': 0,\n",
       " 'interaction_type': 0,\n",
       " 'device_type': 0,\n",
       " 'location': 0,\n",
       " 'price': 0,\n",
       " 'discount': 0,\n",
       " 'product_category': 0,\n",
       " 'brand': 0,\n",
       " 'user_age': 0,\n",
       " 'user_gender': 0,\n",
       " 'loyalty_score': 0,\n",
       " 'previous_purchase_count': 0,\n",
       " 'avg_purchase_value': 0,\n",
       " 'search_keywords': 0,\n",
       " 'rating': 74881,\n",
       " 'purchase': 0}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dict of columns and their null counts\n",
    "null_counts=df.select([F.count(F.when(F.col(c).isNull(),1)).alias(c) for c in df.columns])\n",
    "null_counts.first().asDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28bd500a-9528-4c57-b013-5ceb66c795df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+-----+\n",
      "|purchase|rating|count|\n",
      "+--------+------+-----+\n",
      "|       0|  NULL|71121|\n",
      "|       0|   1.0| 7113|\n",
      "|       0|   2.0|14426|\n",
      "|       0|   3.0|21129|\n",
      "|       0|   4.0|21439|\n",
      "|       0|   5.0| 7195|\n",
      "|       1|  NULL| 3760|\n",
      "|       1|   1.0|  368|\n",
      "|       1|   2.0|  764|\n",
      "|       1|   3.0| 1183|\n",
      "|       1|   4.0| 1145|\n",
      "|       1|   5.0|  359|\n",
      "+--------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"purchase\",\"rating\").count().orderBy(\"purchase\",\"rating\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "63071e80-cc2c-4570-8393-51bbb20fda71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+----+\n",
      "|interaction_type|    0|   1|\n",
      "+----------------+-----+----+\n",
      "|        purchase| NULL|7579|\n",
      "|     add_to_cart|22217|NULL|\n",
      "|            view|75217|NULL|\n",
      "|           click|44989|NULL|\n",
      "+----------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"interaction_type\").pivot(\"purchase\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1805c8-ca44-4547-900e-6ffdb330b6ec",
   "metadata": {},
   "source": [
    "### Results\r\n",
    "\r\n",
    "One column contains a large number of NULL values, while the remaining\r\n",
    "columns are fully populated. Numeric columns do not contain NaN values.\r\n",
    "\r\n",
    "This indicates that missing data is concentrated in a specific field,\r\n",
    "rather than being randomly distributed across the da\n",
    "\n",
    "### Implications for data pipelines\r\n",
    "\r\n",
    "Because NULLs are present, downstream pipelines must explicitly decide\r\n",
    "how to handle missing values for this column.\r\n",
    "\r\n",
    "Depending on how the column is used:\r\n",
    "- reporting pipelines may need to filter or impute NULLs\r\n",
    "- machine learning pipelines may need explicit imputation or masking\r\n",
    "- feature engineering must not assume the field is always present\r\n",
    "\r\n",
    "The following checks should be automated in production:\r\n",
    "- NULL and NaN rates per column\r\n",
    "- NULL rates for critical identifiers and target variables\r\n",
    "\r\n",
    "Pipelines should fail or alert when NULL or NaN rates exceed acceptable\r\n",
    "thresholds, rather than silently propagating missing values.taset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13e87df9-a164-4450-848a-5583f05cde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
