{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a200ce-852f-44d1-9997-9db039bef82b",
   "metadata": {},
   "source": [
    "# Spark Data Analysis Demo\n",
    "This notebook is a demo of how you one can use spark for exploratory data analysis.stems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f209f4-6098-4b6f-9df2-717f997eeb18",
   "metadata": {},
   "source": [
    "## 1. Why this EDA exists\r\n",
    "\r\n",
    "In modern data platforms, data constantly crosses system boundaries:\r\n",
    "from applications to data distributors, from distributors to data lakes,\r\n",
    "from data lakes to warehouses, and from warehouses into analytics\r\n",
    "and machine-learning systems.\r\n",
    "\r\n",
    "At each of these transitions, data can become incomplete\r\n",
    "(e.g. broken exports), duplicative (e.g. repeated ingestimistreatednderstood (e.g. timestamps ingested as strings, numbers parsed as text).\r\n",
    "These issues often propagate silently and eventually break reports,\r\n",
    "analytics, and machine-learning models.\r\n",
    "\r\n",
    "This EDA exissuch detect those risks early, before the data is used\r\n",
    "in product\n",
    "\n",
    "This EDA does not cover ingestion errors (i.e. corrupted files or transmissions), it assumes data parsing completed without errors. ion pipelines. silent data corruption.\r\n",
    " be automated in production pipelines.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d8b3588b-5baf-430b-b2ec-b7d42291eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To explore data we need a spark session. Its and an object used to read data, explore data (including not limited to sql), access spark confirguration. \n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"spark-dq-demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aaae1640-fec2-4548-b2a0-6b3d11f96b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets read a csv file into spark data frame, inferSchema will detect datatypes (if consistent), header will take first row as header.\n",
    "df = spark.read.csv(\n",
    "    \"../data/retail_personalization_dataset.csv\",\n",
    "    inferSchema=True,\n",
    "    header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c3356488-1f3a-4bea-866e-694828746a16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- interaction_type: string (nullable = true)\n",
      " |-- device_type: string (nullable = true)\n",
      " |-- location: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- discount: integer (nullable = true)\n",
      " |-- product_category: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- user_age: integer (nullable = true)\n",
      " |-- user_gender: string (nullable = true)\n",
      " |-- loyalty_score: integer (nullable = true)\n",
      " |-- previous_purchase_count: integer (nullable = true)\n",
      " |-- avg_purchase_value: double (nullable = true)\n",
      " |-- search_keywords: string (nullable = true)\n",
      " |-- rating: double (nullable = true)\n",
      " |-- purchase: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let's print schema of the data frame\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74da27a-6e9d-494e-a389-ab55dffde2a9",
   "metadata": {},
   "source": [
    "Because the source is a CSV file, there is no enforced schema or\r\n",
    "nullability contract, and all constraints must be validated in Spark.\r\n",
    "\r\n",
    "Headers were read correctly and key fields such as timestamp, price,\r\n",
    "discount, user_age, loyalty_score, previous_purchase_count,\r\n",
    "avg_purchase_value, rating, and purchase were correctly inferred as\r\n",
    "numeric or temporal types and not silently downgraded to strings due to\r\n",
    "inconsistent formatting.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057c4112-4bf6-4fa7-88b2-18e1835d11f4",
   "metadata": {},
   "source": [
    "## 3. EEvent-level uniqueness and duplicate risk\n",
    "\n",
    "Before aggregating, joining, or building features, we must understand\n",
    "what a single row represents and whether rows are duplicated.\n",
    "\n",
    "In this dataset, each row is assumed to represent a single user\r\n",
    "interaction with a product at a given point in time.\r\n",
    "\r\n",
    "To validate this assumption, we perform two checks:\r\n",
    "\r\n",
    "- **Full-row duplicates**  \r\n",
    "  We check whether identical rows appear more than once. The presence\r\n",
    "  of full duplicates would indicate upstream ingestion or replay issues.\r\n",
    "\r\n",
    "- **Event-level duplicates**  \r\n",
    "  We check for duplicates using a proxy event identifier:\r\n",
    "  `(user_id, product_id, session_id, timestamp)`.  \r\n",
    "  If this combination is not unique, it suggests that the same real-world\r\n",
    "  interaction has been recorded more than once.\r\n",
    "\r\n",
    "Duplicates at either level would cause overcounting in reports,\r\n",
    "distort aggregated metrics, and introduce bias into machine-learning\r\n",
    "features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68e0b806-3786-4a2c-b49e-00b684b42f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150002"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_rows=df.count()\n",
    "total_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22457a3c-52ba-4183-bf10-9fcf24c9d873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distinct_rows=df.dropDuplicates().count()\n",
    "distinct_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db1e64ae-5269-4f52-bbb7-9abd2a961fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "duplicate_rows = total_rows - distinct_rows\n",
    "duplicate_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc024975-8ae2-41fd-9117-d10ab8e93295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows_keys=df.select(\"user_id\",\"product_id\",\"timestamp\",\"session_id\").dropDuplicates().count()\n",
    "rows_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f34f07-0e01-4231-ba5d-fd982d4fdd64",
   "metadata": {},
   "source": [
    "### Results\r\n",
    "\r\n",
    "The dataset contains:\r\n",
    "- 2 fully duplicated rows\r\n",
    "- 2 duplicated event records when using the proxy\r\n",
    "  `(user_id, product_id, session_id, timestamp)`\r\n",
    "\r\n",
    "This indicates that the same real-world interaction has been recorded\r\n",
    "more than once.\r\n",
    "\r\n",
    "While the number of duplicates is small, their presence confirms that\r\n",
    "duplicate events are a realistic failure mode and must be handled by\r\n",
    "t\n",
    "\n",
    "### Implications for data pipelines\r\n",
    "\r\n",
    "Because duplicate events exist and the dataset contains sufficient\r\n",
    "identifiers to detect them, this data can be made safe for downstream\r\n",
    "use by applying deterministic deduplication during ingestion.\r\n",
    "\r\n",
    "The following checks should be automated in production:\r\n",
    "- detection of fully duplicated rows\r\n",
    "- detection of duplicated event identifiers\r\n",
    "\r\n",
    "Ingestion should either:\r\n",
    "- drop duplicates, or\r\n",
    "- fail when duplicate rates exceed an acceptable thresholdhe pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13e87df9-a164-4450-848a-5583f05cde36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
